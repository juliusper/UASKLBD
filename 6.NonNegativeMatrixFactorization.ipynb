--2023-01-05 11:04:34--  https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip
Resolving github.com (github.com)... 20.27.177.113
Connecting to github.com (github.com)|20.27.177.113|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip [following]
--2023-01-05 11:04:34--  https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 15312323 (15M) [application/zip]
Saving to: ‘recsys.zip’

recsys.zip          100%[===================>]  14.60M  --.-KB/s    in 0.1s    

2023-01-05 11:04:36 (111 MB/s) - ‘recsys.zip’ saved [15312323/15312323]

Archive:  recsys.zip
   creating: recsys/
  inflating: recsys/datasets.py      
  inflating: recsys/preprocessing.py  
  inflating: recsys/utils.py         
  inflating: recsys/requirements.txt  
   creating: recsys/.vscode/
  inflating: recsys/.vscode/settings.json  
   creating: recsys/__pycache__/
  inflating: recsys/__pycache__/datasets.cpython-36.pyc  
  inflating: recsys/__pycache__/datasets.cpython-37.pyc  
  inflating: recsys/__pycache__/utils.cpython-36.pyc  
  inflating: recsys/__pycache__/preprocessing.cpython-37.pyc  
  inflating: recsys/__pycache__/datasets.cpython-38.pyc  
  inflating: recsys/__pycache__/preprocessing.cpython-36.pyc  
  inflating: recsys/__pycache__/preprocessing.cpython-38.pyc  
   creating: recsys/memories/
  inflating: recsys/memories/ItemToItem.py  
  inflating: recsys/memories/UserToUser.py  
   creating: recsys/memories/__pycache__/
  inflating: recsys/memories/__pycache__/UserToUser.cpython-36.pyc  
  inflating: recsys/memories/__pycache__/UserToUser.cpython-37.pyc  
  inflating: recsys/memories/__pycache__/ItemToItem.cpython-37.pyc  
  inflating: recsys/memories/__pycache__/user2user.cpython-36.pyc  
  inflating: recsys/memories/__pycache__/ItemToItem.cpython-36.pyc  
   creating: recsys/models/
  inflating: recsys/models/SVD.py    
  inflating: recsys/models/MatrixFactorization.py  
  inflating: recsys/models/ExplainableMF.py  
  inflating: recsys/models/NonnegativeMF.py  
   creating: recsys/models/__pycache__/
  inflating: recsys/models/__pycache__/SVD.cpython-36.pyc  
  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-37.pyc  
  inflating: recsys/models/__pycache__/ExplainableMF.cpython-36.pyc  
  inflating: recsys/models/__pycache__/ExplainableMF.cpython-37.pyc  
  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-36.pyc  
   creating: recsys/metrics/
  inflating: recsys/metrics/EvaluationMetrics.py  
   creating: recsys/img/
  inflating: recsys/img/MF-and-NNMF.png  
  inflating: recsys/img/svd.png      
  inflating: recsys/img/MF.png       
   creating: recsys/predictions/
   creating: recsys/predictions/item2item/
   creating: recsys/weights/
   creating: recsys/weights/item2item/
   creating: recsys/weights/item2item/ml1m/
  inflating: recsys/weights/item2item/ml1m/similarities.npy  
  inflating: recsys/weights/item2item/ml1m/neighbors.npy  
   creating: recsys/weights/item2item/ml100k/
  inflating: recsys/weights/item2item/ml100k/similarities.npy  
  inflating: recsys/weights/item2item/ml100k/neighbors.npy  
Install and import useful packages
requirements
matplotlib==3.2.2
numpy==1.19.2
pandas==1.0.5
python==3.7
scikit-learn==0.24.1
scikit-surprise==1.1.1
scipy==1.6.2
[2]
1s
from recsys.preprocessing import mean_ratings
from recsys.preprocessing import normalized_ratings
from recsys.preprocessing import ids_encoder
from recsys.preprocessing import train_test_split
from recsys.preprocessing import rating_matrix
from recsys.preprocessing import get_examples
from recsys.preprocessing import scale_ratings

from recsys.datasets import ml1m
from recsys.datasets import ml100k

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

import os
Load and preprocess rating
[3]
4s
# load data
ratings, movies = ml100k.load()

# prepare data
ratings, uencoder, iencoder = ids_encoder(ratings)

# convert ratings from dataframe to numpy array
np_ratings = ratings.to_numpy()

# get examples as tuples of userids and itemids and labels from normalize ratings
raw_examples, raw_labels = get_examples(ratings, labels_column="rating")

# train test split
(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)
Download data 100.2%
Successfully downloaded ml-100k.zip 4924029 bytes.
Unzipping the ml-100k.zip zip file ...
Non-negative Matrix Factorization Model
[4]
0s
class NMF:
    
    def __init__(self, ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.01, lambda_Q=0.01):
        
        np.random.seed(32)
        
        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt
        self.ratings = ratings
        self.np_ratings = ratings.to_numpy()
        self.K = K
        self.P = np.random.rand(m, K)
        self.Q = np.random.rand(n, K)
        
        # hyper parameter initialization
        self.lambda_P = lambda_P
        self.lambda_Q = lambda_Q

        # initialize encoders
        self.uencoder = uencoder
        self.iencoder = iencoder
        
        # training history
        self.history = {
            "epochs": [],
            "loss": [],
            "val_loss": [],
        }
    
    def print_training_parameters(self):
        print('Training NMF ...')
        print(f'k={self.K}')
        
    def mae(self, x_train, y_train):
        """
        returns the Mean Absolute Error
        """
        # number of training examples
        m = x_train.shape[0]
        error = 0
        for pair, r in zip(x_train, y_train):
            u, i = pair
            error += abs(r - np.dot(self.P[u], self.Q[i]))
        return error / m
    
    def update_rule(self, u, i, error):
        I = self.np_ratings[self.np_ratings[:, 0] == u][:, [1, 2]]
        U = self.np_ratings[self.np_ratings[:, 1] == i][:, [0, 2]]    
                    
        num = self.P[u] * np.dot(self.Q[I[:, 0]].T, I[:, 1])
        dem = np.dot(self.Q[I[:, 0]].T, np.dot(self.P[u], self.Q[I[:, 0]].T)) + self.lambda_P * len(I) * self.P[u]
        self.P[u] = num / dem

        num = self.Q[i] * np.dot(self.P[U[:, 0]].T, U[:, 1])
        dem = np.dot(self.P[U[:, 0]].T, np.dot(self.P[U[:, 0]], self.Q[i].T)) + self.lambda_Q * len(U) * self.Q[i]
        self.Q[i] = num / dem
    
    @staticmethod
    def print_training_progress(epoch, epochs, error, val_error, steps=5):
        if epoch == 1 or epoch % steps == 0:
            print(f"epoch {epoch}/{epochs} - loss : {round(error, 3)} - val_loss : {round(val_error, 3)}")
                
    def fit(self, x_train, y_train, validation_data, epochs=10):

        self.print_training_parameters()
        x_test, y_test = validation_data
        for epoch in range(1, epochs+1):
            for pair, r in zip(x_train, y_train):
                u, i = pair
                r_hat = np.dot(self.P[u], self.Q[i])
                e = abs(r - r_hat)
                self.update_rule(u, i, e)                
            # training and validation error  after this epochs
            error = self.mae(x_train, y_train)
            val_error = self.mae(x_test, y_test)
            self.update_history(epoch, error, val_error)
            self.print_training_progress(epoch, epochs, error, val_error, steps=1)
        
        return self.history
    
    def update_history(self, epoch, error, val_error):
        self.history['epochs'].append(epoch)
        self.history['loss'].append(error)
        self.history['val_loss'].append(val_error)
    
    def evaluate(self, x_test, y_test):        
        error = self.mae(x_test, y_test)
        print(f"validation error : {round(error,3)}")
        print('MAE : ', error)        
        return error
      
    def predict(self, userid, itemid):
        u = self.uencoder.transform([userid])[0]
        i = self.iencoder.transform([itemid])[0]
        r = np.dot(self.P[u], self.Q[i])
        return r

    def recommend(self, userid, N=30):
        # encode the userid
        u = self.uencoder.transform([userid])[0]

        # predictions for users userid on all product
        predictions = np.dot(self.P[u], self.Q.T)

        # get the indices of the top N predictions
        top_idx = np.flip(np.argsort(predictions))[:N]

        # decode indices to get their corresponding itemids
        top_items = self.iencoder.inverse_transform(top_idx)

        # take corresponding predictions for top N indices
        preds = predictions[top_idx]

        return top_items, preds

Train the NMF model with ML-100K dataset
model parameters :

k=10 : (number of factors)
λP=0.6
λQ=0.6
epochs = 10
Note that it may take some time to complete the training on 10 epochs (around 7 minutes).

[5]
11m
m = ratings['userid'].nunique()   # total number of users
n = ratings['itemid'].nunique()   # total number of items

# create and train the model
nmf = NMF(ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.6, lambda_Q=0.6)
history = nmf.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))
Training NMF ...
k=10
epoch 1/10 - loss : 0.916 - val_loss : 0.917
epoch 2/10 - loss : 0.915 - val_loss : 0.917
epoch 3/10 - loss : 0.915 - val_loss : 0.917
epoch 4/10 - loss : 0.915 - val_loss : 0.917
epoch 5/10 - loss : 0.915 - val_loss : 0.917
epoch 6/10 - loss : 0.915 - val_loss : 0.917
epoch 7/10 - loss : 0.915 - val_loss : 0.917
epoch 8/10 - loss : 0.915 - val_loss : 0.917
epoch 9/10 - loss : 0.915 - val_loss : 0.917
epoch 10/10 - loss : 0.915 - val_loss : 0.917
[6]
0s
nmf.evaluate(x_test, y_test)
validation error : 0.917
MAE :  0.9165041343019539
0.9165041343019539
Evaluation of NMF with Scikit-suprise
We can use the scikt-suprise package to train the NMF model. It is an easy-to-use Python scikit for recommender systems.

Import the NMF class from the suprise scikit.
Load the data with the built-in function
Instanciate NMF with k=10 (n_factors) and we use 10 epochs (n_epochs)
Evaluate the model using cross-validation with 5 folds.
[7]
0s
from surprise import NMF
from surprise import Dataset
from surprise.model_selection import cross_validate

# Load the movielens-100k dataset (download it if needed).
data = Dataset.load_builtin('ml-100k')

# Use the NMF algorithm.
nmf = NMF(n_factors=10, n_epochs=10)

# Run 5-fold cross-validation and print results.
history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)

As result, the mean MAE on the test set is mae = 0.9510 which is equivalent to the result we have obtained on ml-100k with our own implementation mae = 0.9165

ML-1M
The mean MAE on a 5-fold cross-validation is mae = 0.9567

This may take arount 2 minutes

[ ]
0s
data = Dataset.load_builtin('ml-1m')
nmf = NMF(n_factors=10, n_epochs=10)
history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)
Explainable Matrix Factorization
NMF introduced explainability to MF by constraining P and Q values in [0,1]. It can be considered is an inner explainable model. It's possible to inject external informations into the model in order to bring explainability. This is what the Explainable Matrix Factorization (EMF) algorithm does. It computes explainable scores from user or item similarities taken from the user-based or item-based CF.

Click here to open the EMF notebook.

Reference
Daniel D. Lee & H. Sebastian Seung (1999). Learning the parts of objects by non-negative matrix factorization
Deng Cai et al. (2008). Non-negative Matrix Factorization on Manifold
Yu-Xiong Wang and Yu-Jin Zhang (2011). Non-negative Matrix Factorization: a Comprehensive Review
Nicolas Gillis (2014). The Why and How of Nonnegative Matrix Factorization
Author
Carmel WENGA,
PhD student at Université de la Polynésie Française,
Applied Machine Learning Research Engineer,
ShoppingList, NzhinuSoft.


